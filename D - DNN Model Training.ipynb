{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1432a93a-6e86-42d7-93e6-231aa3eb1b56",
   "metadata": {},
   "source": [
    "# D - Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5813a762-aea5-4af2-8a13-9e139d21ff70",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c0c0206-3536-4271-a227-1259d016f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import base64\n",
    "import csv\n",
    "import joblib\n",
    "from dnn_models import *\n",
    "\n",
    "def group_features(df):\n",
    "    feature_groups = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col.startswith(\"DNN_\"):  # Filtra apenas colunas que começam com \"DNN_\"\n",
    "            parts = col.split(\"_\")\n",
    "            if len(parts) > 2:  # Garante que há pelo menos um conjunto intermediário\n",
    "                base_feature = \"_\".join(parts[1:-1])  # Pega todos os conjuntos entre o primeiro e o último\n",
    "                \n",
    "                if base_feature not in feature_groups:\n",
    "                    feature_groups[base_feature] = []\n",
    "                \n",
    "                feature_groups[base_feature].append(col)\n",
    "\n",
    "    return feature_groups\n",
    "\n",
    "\n",
    "def generate_combinations_and_csv(features, max_length, output_dir, model_prefix, csv_filename):\n",
    "    \"\"\"\n",
    "    Generate all combinations of features from size 1 to `max_length`, and save the results to a CSV file.\n",
    "    \n",
    "    Each combination includes:\n",
    "    - The formatted combination (e.g., \"feature_1 / feature_2\").\n",
    "    - The corresponding model file path.\n",
    "    - The corresponding metrics file path.\n",
    "    \n",
    "    Args:\n",
    "        features (list): List of feature names (strings).\n",
    "        max_length (int): Maximum size of combinations to generate.\n",
    "        output_dir (str): Directory where the output files will be saved.\n",
    "        model_prefix (str): Prefix for naming model and metrics files.\n",
    "        csv_filename (str): Name of the output CSV file.\n",
    "    \n",
    "    Output:\n",
    "        A CSV file containing the combinations, model file paths, and metrics file paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the output directories exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    models_dir = os.path.join(output_dir, \"models\")\n",
    "    metrics_dir = os.path.join(output_dir, \"metrics\")\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    os.makedirs(metrics_dir, exist_ok=True)\n",
    "\n",
    "    # List to store rows for the CSV file\n",
    "    csv_rows = []\n",
    "\n",
    "    # Generate all combinations of sizes from 1 to `max_length`\n",
    "    for size in range(1, max_length + 1):\n",
    "        for combination in itertools.combinations(features, size):\n",
    "            # Format the combination as \"feature_1 / feature_2 / ...\"\n",
    "            formatted_combination = \" / \".join(combination)\n",
    "\n",
    "            # Create the Base64-encoded name for the model and metrics files\n",
    "            base_name = \"/\".join(combination)  # Use \"/\" as the separator for encoding\n",
    "            base_name_encoded = base64.urlsafe_b64encode(base_name.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "\n",
    "            # Full paths for the model and metrics files\n",
    "            model_file = os.path.join(models_dir, f\"{model_prefix}_{base_name_encoded}.keras\")\n",
    "            metrics_file = os.path.join(metrics_dir, f\"{model_prefix}_{base_name_encoded}.csv\")\n",
    "\n",
    "            # Add the row to the CSV data\n",
    "            csv_rows.append({\n",
    "                \"combination\": formatted_combination,\n",
    "                \"model_file\": model_file,\n",
    "                \"metrics_file\": metrics_file\n",
    "            })\n",
    "\n",
    "    # Save the CSV file\n",
    "    csv_filepath = os.path.join(output_dir, csv_filename)\n",
    "    with open(csv_filepath, mode=\"w\", encoding=\"utf-8\", newline=\"\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"combination\", \"model_file\", \"metrics_file\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_rows)\n",
    "\n",
    "    print(f\"CSV file successfully generated: {csv_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4569be91-b01e-4cc8-a34a-ac8aa2ee7aff",
   "metadata": {},
   "source": [
    "## Create all possible model combinations using feature sets ranging in length from 1 to 4.\n",
    "\n",
    "12,590 combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadbfb67-4783-4a1b-a4a0-a08189862126",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_val = joblib.load(\"DNN_val.joblib\")\n",
    "\n",
    "feature_groups = list(group_features(DNN_val).keys())\n",
    "max_length = 4\n",
    "output_dir = \"DNN_MODEL_TRAINING\"\n",
    "model_prefix = \"DNN\"\n",
    "csv_filename = \"DNN_models_combination.csv\"\n",
    "\n",
    "generate_combinations_and_csv(feature_groups, max_length, output_dir, model_prefix, csv_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de26693-ecb1-45d9-852b-b32ee75d74b6",
   "metadata": {},
   "source": [
    "## Train all model combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78561e5f-490f-4285-b838-2aea56b23840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, cohen_kappa_score\n",
    "\n",
    "\n",
    "def train_and_evaluate(params, output_dir, feature_groups, train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    Trains and evaluates a single combination of model, feature group, and hyperparameters.\n",
    "    Saves the trained model and metrics if they don't already exist.\n",
    "    \n",
    "    Args:\n",
    "        params (dict): A dictionary containing all necessary parameters for training and evaluation.\n",
    "        output_dir (str): Directory where models and metrics will be saved.\n",
    "        feature_groups (dict): Dictionary mapping feature groups to their corresponding columns.\n",
    "        train_df (pd.DataFrame): Training dataset.\n",
    "        val_df (pd.DataFrame): Validation dataset.\n",
    "        test_df (pd.DataFrame): Test dataset.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Import TensorFlow and other dependencies inside the function\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU, Input\n",
    "    from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "    # Unpack parameters\n",
    "    model_func = params[\"model_func\"]\n",
    "    feature_combination = params[\"feature_group\"]\n",
    "    epochs = params[\"epochs\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    patience = params[\"patience\"]\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    monitor_metric = params[\"monitor_metric\"]\n",
    "    model_name = params[\"model_name\"]\n",
    "    model_path = params[\"model_file\"]  # Model file path from combinations_csv\n",
    "    metrics_path = params[\"metrics_file\"]  # Metrics file path from combinations_csv\n",
    "    print(f\"Working on {feature_combination}\")\n",
    "    \n",
    "    # Check if both model and metrics files already exist\n",
    "    if os.path.exists(model_path) and os.path.exists(metrics_path):\n",
    "        print(f\"Skipping training for {model_path} (both model and metrics already exist)\")\n",
    "        return\n",
    "    \n",
    "    # Filter the datasets based on the feature combination\n",
    "    feature_keys = [key.strip() for key in feature_combination.split(\"/\")]\n",
    "    missing_keys = [key for key in feature_keys if key not in feature_groups]\n",
    "    if missing_keys:\n",
    "        raise KeyError(f\"Missing feature groups in the dictionary: {missing_keys}\")\n",
    "    \n",
    "    columns = []\n",
    "    for key in feature_keys:\n",
    "        columns.extend(feature_groups[key])\n",
    "    \n",
    "    X_train = train_df[columns].values\n",
    "    X_val = val_df[columns].values\n",
    "    X_test = test_df[columns].values\n",
    "    \n",
    "    # Use raw integer targets instead of one-hot encoding\n",
    "    y_train = train_df['emotion'].values\n",
    "    y_val = val_df['emotion'].values\n",
    "    y_test = test_df['emotion'].values\n",
    "    \n",
    "    # Get the number of input features dynamically\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # Determine the number of unique classes\n",
    "    num_classes = len(train_df['emotion'].unique())\n",
    "    \n",
    "    # Create and compile the model\n",
    "    model, optimizer = model_func(input_dim=input_dim, num_classes=num_classes, learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define early stopping callback\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=monitor_metric, patience=patience, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Evaluate the model on the training and validation sets\n",
    "    _, train_accuracy = model.evaluate(X_train, y_train, verbose=1)\n",
    "    _, val_accuracy = model.evaluate(X_val, y_val, verbose=1)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    y_pred = y_pred_proba.argmax(axis=1)\n",
    "    y_true = y_test\n",
    "    \n",
    "    # Calculate additional metrics for the test set (val2)\n",
    "    val2_accuracy = accuracy_score(y_true, y_pred)\n",
    "    val2_recall = recall_score(y_true, y_pred, average='weighted')\n",
    "    val2_precision = precision_score(y_true, y_pred, average='weighted')\n",
    "    val2_f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    val2_confusion_matrix = confusion_matrix(y_true, y_pred).tolist()\n",
    "    val2_cohen_kappa = cohen_kappa_score(y_true, y_pred)\n",
    "    val2_error_indices = [i for i, (true, pred) in enumerate(zip(y_true, y_pred)) if true != pred]\n",
    "    val2_accuracy_vector = [1 if true == pred else 0 for true, pred in zip(y_true, y_pred)]\n",
    "    \n",
    "    # Calculate the gap between train_accuracy and val2_accuracy\n",
    "    gap = train_accuracy - val2_accuracy\n",
    "    \n",
    "    # Save the trained model\n",
    "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "    model.save(model_path)\n",
    "    print(f\"Saved model to {model_path}\")\n",
    "    \n",
    "    # Save the metrics\n",
    "    metrics = {\n",
    "        \"Model\": model_name,\n",
    "        \"Feature Group\": feature_combination,\n",
    "        \"train_accuracy\": train_accuracy,\n",
    "        \"val_accuracy\": val_accuracy,\n",
    "        \"val2_accuracy\": val2_accuracy,\n",
    "        \"gap\": gap,  # Updated gap calculation\n",
    "        \"val2_recall\": val2_recall,\n",
    "        \"val2_precision\": val2_precision,\n",
    "        \"val2_f1\": val2_f1,\n",
    "        \"val2_model_path\": model_path,\n",
    "        \"val2_Confusion_Matrix\": val2_confusion_matrix,\n",
    "        \"val2_Cohen_Kappa_Score\": val2_cohen_kappa,\n",
    "        \"val2_error_indices\": val2_error_indices,\n",
    "        \"val2_accuracy_vector\": val2_accuracy_vector,\n",
    "        \"val2_y_pred\": y_pred.tolist(),\n",
    "        \"val2_y_true\": y_true.tolist(),\n",
    "        \"val2_y_proba\": y_pred_proba.tolist()\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df.to_csv(metrics_path, index=False)\n",
    "    print(f\"Saved metrics to {metrics_path}\")\n",
    "\n",
    "\n",
    "def train_dnn_models(output_dir, train_df, val_df, test_df, combinations_csv, num_workers=10):\n",
    "    \"\"\"\n",
    "    Trains and evaluates the DNN-6 model for each feature combination in the provided CSV file,\n",
    "    using fixed hyperparameters and parallel processing.\n",
    "    \n",
    "    Args:\n",
    "        output_dir (str): Directory where models and metrics will be saved.\n",
    "        train_df (pd.DataFrame): Training dataset.\n",
    "        val_df (pd.DataFrame): Validation dataset.\n",
    "        test_df (pd.DataFrame): Test dataset.\n",
    "        combinations_csv (str): Path to the CSV file containing feature combinations.\n",
    "        num_workers (int): Number of parallel workers.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Define fixed hyperparameters\n",
    "    epochs = 100\n",
    "    batch_size = 64\n",
    "    patience = 10\n",
    "    learning_rate = 0.0005\n",
    "    monitor_metric = 'val_accuracy'\n",
    "    \n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate the global feature groups dictionary\n",
    "    feature_groups = group_features(train_df)\n",
    "    \n",
    "    # Load the combinations CSV file\n",
    "    try:\n",
    "        combinations_df = pd.read_csv(combinations_csv)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error reading combinations CSV file: {e}\")\n",
    "    \n",
    "    # Validate that the required columns exist\n",
    "    required_columns = [\"combination\", \"model_file\", \"metrics_file\"]\n",
    "    missing_columns = [col for col in required_columns if col not in combinations_df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"The combinations CSV file is missing the following columns: {missing_columns}\")\n",
    "    \n",
    "    # Prepare all experiments\n",
    "    experiments = []\n",
    "    for _, row in combinations_df.iterrows():\n",
    "        feature_combination = row['combination']\n",
    "        model_file = row['model_file']\n",
    "        metrics_file = row['metrics_file']\n",
    "        \n",
    "        # Append the experiment configuration\n",
    "        experiments.append({\n",
    "            \"model_func\": create_model_6,  # Only DNN-6 is used\n",
    "            \"feature_group\": feature_combination,  # Use the combination as the feature group name\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"patience\": patience,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"monitor_metric\": monitor_metric,\n",
    "            \"model_name\": \"DNN-6\",  # Fixed model name\n",
    "            \"model_file\": model_file,  # Model file path from combinations_csv\n",
    "            \"metrics_file\": metrics_file  # Metrics file path from combinations_csv\n",
    "        })\n",
    "    \n",
    "    # Use multiprocessing to run experiments in parallel\n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        # Map the experiments to the worker function\n",
    "        pool.starmap(\n",
    "            train_and_evaluate,\n",
    "            [(exp, output_dir, feature_groups, train_df, val_df, test_df) for exp in experiments]\n",
    "        )\n",
    "\n",
    "    print(\"All experiments completed. Models and metrics saved individually.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa7f3c-45e8-48a5-8f39-1473e058a58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DNN_train = joblib.load(\"DNN_train.joblib\")\n",
    "DNN_val = joblib.load(\"DNN_val.joblib\")\n",
    "DNN_val2 = joblib.load(\"DNN_val2.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5bfb6-accb-47c7-9e1f-5a322c94931b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir = \"DNN_MODEL_TRAINING\"\n",
    "csv_filename = os.path.join(output_dir, \"DNN_models_combination.csv\")\n",
    "\n",
    "train_dnn_models(output_dir, DNN_train, DNN_val, DNN_val2, csv_filename, num_workers=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84eb4d61-b1cd-4527-9a6c-f460ba62160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def consolidate_csv(directory: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Traverses a directory and its subdirectories to find all CSV files,\n",
    "    consolidating them into a single destination file.\n",
    "    \n",
    "    Parameters:\n",
    "    directory (str): Path to the root directory where CSV files are located.\n",
    "    output_file (str): Path to the destination file where consolidated data will be saved.\n",
    "    \"\"\"\n",
    "    all_dfs = []  # List to store temporary DataFrames\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):  # Check if the file is a CSV\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # Read the CSV file into a DataFrame\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    df['model_file'] = file\n",
    "                    all_dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Consolidate all DataFrames into a single one and save to the destination file\n",
    "    if all_dfs:\n",
    "        consolidated_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        consolidated_df.to_csv(output_file, index=False)\n",
    "        print(f\"Consolidation completed. File saved at: {output_file}\")\n",
    "    else:\n",
    "        print(\"No CSV files found for consolidation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49e4e48-df60-4e83-a496-fa5cf8a1e0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidation completed. File saved at: DNN_MODEL_TRAINING/DNN_models_combination_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "consolidate_csv(\"DNN_MODEL_TRAINING/metrics\",\"DNN_MODEL_TRAINING/DNN_models_combination_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa99e0f4-a0e0-453d-ad6a-064a6908a979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
